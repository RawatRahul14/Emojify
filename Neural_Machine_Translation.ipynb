{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXDP7z_-2s84",
        "outputId": "b3f3ba3f-1631-4af3-db68-da22d0db6bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.10/dist-packages (19.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Gu-1UfE2jPd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from faker import Faker\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from babel.dates import format_date\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fake = Faker()\n",
        "Faker.seed(12345)\n",
        "random.seed(12345)\n",
        "\n",
        "# Define format of the data we would like to generate\n",
        "FORMATS = ['short',\n",
        "           'medium',\n",
        "           'long',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'd MMM YYY',\n",
        "           'd MMMM YYY',\n",
        "           'dd MMM YYY',\n",
        "           'd MMM, YYY',\n",
        "           'd MMMM, YYY',\n",
        "           'dd, MMM YYY',\n",
        "           'd MM YY',\n",
        "           'd MMMM YYY',\n",
        "           'MMMM d YYY',\n",
        "           'MMMM d, YYY',\n",
        "           'dd.MM.YY']\n",
        "\n",
        "# change this if you want it to work with another language\n",
        "LOCALES = ['en_US']\n",
        "\n",
        "def load_date():\n",
        "    \"\"\"\n",
        "        Loads some fake dates\n",
        "        :returns: tuple containing human readable string, machine readable string, and date object\n",
        "    \"\"\"\n",
        "    dt = fake.date_object()\n",
        "\n",
        "    try:\n",
        "        human_readable = format_date(dt, format=random.choice(FORMATS),  locale='en_US') # locale=random.choice(LOCALES))\n",
        "        human_readable = human_readable.lower()\n",
        "        human_readable = human_readable.replace(',','')\n",
        "        machine_readable = dt.isoformat()\n",
        "\n",
        "    except AttributeError as e:\n",
        "        return None, None, None\n",
        "\n",
        "    return human_readable, machine_readable, dt\n",
        "\n",
        "def load_dataset(m):\n",
        "    \"\"\"\n",
        "        Loads a dataset with m examples and vocabularies\n",
        "        :m: the number of examples to generate\n",
        "    \"\"\"\n",
        "\n",
        "    human_vocab = set()\n",
        "    machine_vocab = set()\n",
        "    dataset = []\n",
        "    Tx = 30\n",
        "\n",
        "\n",
        "    for i in tqdm(range(m)):\n",
        "        h, m, _ = load_date()\n",
        "        if h is not None:\n",
        "            dataset.append((h, m))\n",
        "            human_vocab.update(tuple(h))\n",
        "            machine_vocab.update(tuple(m))\n",
        "\n",
        "    human = dict(zip(sorted(human_vocab) + ['<unk>', '<pad>'],\n",
        "                     list(range(len(human_vocab) + 2))))\n",
        "    inv_machine = dict(enumerate(sorted(machine_vocab)))\n",
        "    machine = {v:k for k,v in inv_machine.items()}\n",
        "\n",
        "    return dataset, human, machine, inv_machine\n",
        "\n",
        "def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n",
        "\n",
        "    X, Y = zip(*dataset)\n",
        "\n",
        "    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n",
        "    Y = [string_to_int(t, Ty, machine_vocab) for t in Y]\n",
        "\n",
        "    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n",
        "    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n",
        "\n",
        "\n",
        "\n",
        "    return X, np.array(Y), Xoh, Yoh\n",
        "\n",
        "def string_to_int(string, length, vocab):\n",
        "    \"\"\"\n",
        "    Converts all strings in the vocabulary into a list of integers representing the positions of the\n",
        "    input string's characters in the \"vocab\"\n",
        "\n",
        "    Arguments:\n",
        "    string -- input string, e.g. 'Wed 10 Jul 2007'\n",
        "    length -- the number of time steps you'd like, determines if the output will be padded or cut\n",
        "    vocab -- vocabulary, dictionary used to index every character of your \"string\"\n",
        "\n",
        "    Returns:\n",
        "    rep -- list of integers (or '<unk>') (size = length) representing the position of the string's character in the vocabulary\n",
        "    \"\"\"\n",
        "\n",
        "    #make lower to standardize\n",
        "    string = string.lower()\n",
        "    string = string.replace(',','')\n",
        "\n",
        "    if len(string) > length:\n",
        "        string = string[:length]\n",
        "\n",
        "    rep = list(map(lambda x: vocab.get(x, '<unk>'), string))\n",
        "\n",
        "    if len(string) < length:\n",
        "        rep += [vocab['<pad>']] * (length - len(string))\n",
        "\n",
        "    #print (rep)\n",
        "    return rep\n",
        "\n",
        "\n",
        "def int_to_string(ints, inv_vocab):\n",
        "    \"\"\"\n",
        "    Output a machine readable list of characters based on a list of indexes in the machine's vocabulary\n",
        "\n",
        "    Arguments:\n",
        "    ints -- list of integers representing indexes in the machine's vocabulary\n",
        "    inv_vocab -- dictionary mapping machine readable indexes to machine readable characters\n",
        "\n",
        "    Returns:\n",
        "    l -- list of characters corresponding to the indexes of ints thanks to the inv_vocab mapping\n",
        "    \"\"\"\n",
        "\n",
        "    l = [inv_vocab[i] for i in ints]\n",
        "    return l\n",
        "\n",
        "\n",
        "EXAMPLES = ['3 May 1979', '5 Apr 09', '20th February 2016', 'Wed 10 Jul 2007']\n",
        "\n",
        "def run_example(model, input_vocabulary, inv_output_vocabulary, text):\n",
        "    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n",
        "    prediction = model.predict(np.array([encoded]))\n",
        "    prediction = np.argmax(prediction[0], axis=-1)\n",
        "    return int_to_string(prediction, inv_output_vocabulary)\n",
        "\n",
        "def run_examples(model, input_vocabulary, inv_output_vocabulary, examples=EXAMPLES):\n",
        "    predicted = []\n",
        "    for example in examples:\n",
        "        predicted.append(''.join(run_example(model, input_vocabulary, inv_output_vocabulary, example)))\n",
        "        print('input:', example)\n",
        "        print('output:', predicted[-1])\n",
        "    return predicted\n",
        "\n",
        "\n",
        "def softmax(x, axis=1):\n",
        "    \"\"\"Softmax activation function.\n",
        "    # Arguments\n",
        "        x : Tensor.\n",
        "        axis: Integer, axis along which the softmax normalization is applied.\n",
        "    # Returns\n",
        "        Tensor, output of softmax transformation.\n",
        "    # Raises\n",
        "        ValueError: In case `dim(x) == 1`.\n",
        "    \"\"\"\n",
        "    ndim = K.ndim(x)\n",
        "    if ndim == 2:\n",
        "        return K.softmax(x)\n",
        "    elif ndim > 2:\n",
        "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
        "        s = K.sum(e, axis=axis, keepdims=True)\n",
        "        return e / s\n",
        "    else:\n",
        "        raise ValueError('Cannot apply softmax to a tensor that is 1D')\n",
        "\n",
        "\n",
        "def plot_attention_map(modelx, input_vocabulary, inv_output_vocabulary, text, n_s = 128, num = 7):\n",
        "    \"\"\"\n",
        "    Plot the attention map.\n",
        "\n",
        "    \"\"\"\n",
        "    attention_map = np.zeros((10, 30))\n",
        "    layer = modelx.get_layer('attention_weights')\n",
        "\n",
        "    Ty, Tx = attention_map.shape\n",
        "\n",
        "    human_vocab_size = 37\n",
        "\n",
        "    # Well, this is cumbersome but this version of tensorflow-keras has a bug that affects the\n",
        "    # reuse of layers in a model with the functional API.\n",
        "    # So, I have to recreate the model based on the functional\n",
        "    # components and connect then one by one.\n",
        "    # ideally it can be done simply like this:\n",
        "    # layer = modelx.layers[num]\n",
        "    # f = Model(modelx.inputs, [layer.get_output_at(t) for t in range(Ty)])\n",
        "    #\n",
        "\n",
        "    X = modelx.inputs[0]\n",
        "    s0 = modelx.inputs[1]\n",
        "    c0 = modelx.inputs[2]\n",
        "    s = s0\n",
        "    c = s0\n",
        "\n",
        "    a = modelx.layers[2](X)\n",
        "    outputs = []\n",
        "\n",
        "    for t in range(Ty):\n",
        "        s_prev = s\n",
        "        s_prev = modelx.layers[3](s_prev)\n",
        "        concat = modelx.layers[4]([a, s_prev])\n",
        "        e = modelx.layers[5](concat)\n",
        "        energies = modelx.layers[6](e)\n",
        "        alphas = modelx.layers[7](energies)\n",
        "        context = modelx.layers[8]([alphas, a])\n",
        "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
        "        s, _, c = modelx.layers[10](context, initial_state = [s, c])\n",
        "        outputs.append(energies)\n",
        "\n",
        "    f = Model(inputs=[X, s0, c0], outputs = outputs)\n",
        "\n",
        "\n",
        "    s0 = np.zeros((1, n_s))\n",
        "    c0 = np.zeros((1, n_s))\n",
        "    encoded = np.array(string_to_int(text, Tx, input_vocabulary)).reshape((1, 30))\n",
        "    encoded = np.array(list(map(lambda x: to_categorical(x, num_classes=len(input_vocabulary)), encoded)))\n",
        "\n",
        "\n",
        "    r = f([encoded, s0, c0])\n",
        "\n",
        "    for t in range(Ty):\n",
        "        for t_prime in range(Tx):\n",
        "            attention_map[t][t_prime] = r[t][0, t_prime]\n",
        "\n",
        "    # Normalize attention map\n",
        "    row_max = attention_map.max(axis=1)\n",
        "    attention_map = attention_map / row_max[:, None]\n",
        "\n",
        "    prediction = modelx.predict([encoded, s0, c0])\n",
        "\n",
        "    predicted_text = []\n",
        "    for i in range(len(prediction)):\n",
        "        predicted_text.append(int(np.argmax(prediction[i], axis=1)))\n",
        "\n",
        "    predicted_text = list(predicted_text)\n",
        "    predicted_text = int_to_string(predicted_text, inv_output_vocabulary)\n",
        "    text_ = list(text)\n",
        "\n",
        "    # get the lengths of the string\n",
        "    input_length = len(text)\n",
        "    output_length = Ty\n",
        "\n",
        "    # Plot the attention_map\n",
        "    plt.clf()\n",
        "    f = plt.figure(figsize=(8, 8.5))\n",
        "    ax = f.add_subplot(1, 1, 1)\n",
        "\n",
        "    # add image\n",
        "    i = ax.imshow(attention_map, interpolation='nearest', cmap='Blues')\n",
        "\n",
        "    # add colorbar\n",
        "    cbaxes = f.add_axes([0.2, 0, 0.6, 0.03])\n",
        "    cbar = f.colorbar(i, cax=cbaxes, orientation='horizontal')\n",
        "    cbar.ax.set_xlabel('Alpha value (Probability output of the \"softmax\")', labelpad=2)\n",
        "\n",
        "    # add labels\n",
        "    ax.set_yticks(range(output_length))\n",
        "    ax.set_yticklabels(predicted_text[:output_length])\n",
        "\n",
        "    ax.set_xticks(range(input_length))\n",
        "    ax.set_xticklabels(text_[:input_length], rotation=45)\n",
        "\n",
        "    ax.set_xlabel('Input Sequence')\n",
        "    ax.set_ylabel('Output Sequence')\n",
        "\n",
        "    # add grid and legend\n",
        "    ax.grid()\n",
        "\n",
        "    #f.show()\n",
        "\n",
        "    return attention_map"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from termcolor import colored\n",
        "\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.layers import ZeroPadding2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import RepeatVector\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Compare the two inputs\n",
        "def comparator(learner, instructor):\n",
        "    layer = 0\n",
        "    for a, b in zip(learner, instructor):\n",
        "        if tuple(a) != tuple(b):\n",
        "            print(colored(\"Test failed\", attrs=['bold']),\n",
        "                  f\"at layer: {layer}\",\n",
        "                  \"\\n Expected value \\n\\n\", colored(f\"{b}\", \"green\"),\n",
        "                  \"\\n\\n does not match the input value: \\n\\n\",\n",
        "                  colored(f\"{a}\", \"red\"))\n",
        "            raise AssertionError(\"Error in test\")\n",
        "        layer += 1\n",
        "    print(colored(\"All tests passed!\", \"green\"))\n",
        "\n",
        "# extracts the description of a given model\n",
        "def summary(model):\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    result = []\n",
        "    for layer in model.layers:\n",
        "        descriptors = [layer.__class__.__name__, layer.output_shape, layer.count_params()]\n",
        "        if (type(layer) == Conv2D):\n",
        "            descriptors.append(layer.padding)\n",
        "            descriptors.append(layer.activation.__name__)\n",
        "            descriptors.append(layer.kernel_initializer.__class__.__name__)\n",
        "        if (type(layer) == MaxPooling2D):\n",
        "            descriptors.append(layer.pool_size)\n",
        "            descriptors.append(layer.strides)\n",
        "            descriptors.append(layer.padding)\n",
        "        if (type(layer) == Dropout):\n",
        "            descriptors.append(layer.rate)\n",
        "        if (type(layer) == ZeroPadding2D):\n",
        "            descriptors.append(layer.padding)\n",
        "        if (type(layer) == Dense):\n",
        "            descriptors.append(layer.activation.__name__)\n",
        "        if (type(layer) == LSTM):\n",
        "            descriptors.append(layer.input_shape)\n",
        "            descriptors.append(layer.activation.__name__)\n",
        "        if (type(layer) == RepeatVector):\n",
        "            descriptors.append(layer.n)\n",
        "        result.append(descriptors)\n",
        "    return result"
      ],
      "metadata": {
        "id": "eF5dwTl9Ub0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
        "from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from faker import Faker\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from babel.dates import format_date\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "-fb7gTAa2oMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = 10000\n",
        "\n",
        "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGGg-i3Y23PV",
        "outputId": "d16d11a3-f9fe-4454-e3e3-b153690d60e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:00<00:00, 21944.30it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMocd3d73GxO",
        "outputId": "d5f13409-4303-495e-81d3-a50b0fb9f977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('9 may 1998', '1998-05-09'),\n",
              " ('10.11.19', '2019-11-10'),\n",
              " ('9/10/70', '1970-09-10'),\n",
              " ('saturday april 28 1990', '1990-04-28'),\n",
              " ('thursday january 26 1995', '1995-01-26'),\n",
              " ('monday march 7 1983', '1983-03-07'),\n",
              " ('sunday may 22 1988', '1988-05-22'),\n",
              " ('08 jul 2008', '2008-07-08'),\n",
              " ('8 sep 1999', '1999-09-08'),\n",
              " ('thursday january 1 1981', '1981-01-01')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "human_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-0OM-Fe3Kcc",
        "outputId": "25b79a7d-6b05-49fa-d2a8-1e1210293dd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ': 0,\n",
              " '.': 1,\n",
              " '/': 2,\n",
              " '0': 3,\n",
              " '1': 4,\n",
              " '2': 5,\n",
              " '3': 6,\n",
              " '4': 7,\n",
              " '5': 8,\n",
              " '6': 9,\n",
              " '7': 10,\n",
              " '8': 11,\n",
              " '9': 12,\n",
              " 'a': 13,\n",
              " 'b': 14,\n",
              " 'c': 15,\n",
              " 'd': 16,\n",
              " 'e': 17,\n",
              " 'f': 18,\n",
              " 'g': 19,\n",
              " 'h': 20,\n",
              " 'i': 21,\n",
              " 'j': 22,\n",
              " 'l': 23,\n",
              " 'm': 24,\n",
              " 'n': 25,\n",
              " 'o': 26,\n",
              " 'p': 27,\n",
              " 'r': 28,\n",
              " 's': 29,\n",
              " 't': 30,\n",
              " 'u': 31,\n",
              " 'v': 32,\n",
              " 'w': 33,\n",
              " 'y': 34,\n",
              " '<unk>': 35,\n",
              " '<pad>': 36}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "machine_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44fE3_Ox3OgP",
        "outputId": "1061e539-fcb7-41e4-c15f-1c47fcb80462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'-': 0,\n",
              " '0': 1,\n",
              " '1': 2,\n",
              " '2': 3,\n",
              " '3': 4,\n",
              " '4': 5,\n",
              " '5': 6,\n",
              " '6': 7,\n",
              " '7': 8,\n",
              " '8': 9,\n",
              " '9': 10}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inv_machine_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUw43T133QHt",
        "outputId": "00aab85a-fa2c-495d-97e0-affde91a8df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '-',\n",
              " 1: '0',\n",
              " 2: '1',\n",
              " 3: '2',\n",
              " 4: '3',\n",
              " 5: '4',\n",
              " 6: '5',\n",
              " 7: '6',\n",
              " 8: '7',\n",
              " 9: '8',\n",
              " 10: '9'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Tx = 30\n",
        "Ty = 10\n",
        "\n",
        "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
        "\n",
        "print(\"X.shape\", X.shape)\n",
        "print(\"Y.shape\", Y.shape)\n",
        "print(\"Xoh.shape\", Xoh.shape)\n",
        "print(\"Yoh.shape\", Yoh.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLD7A2qt3TM1",
        "outputId": "ada450fa-9268-440a-8a50-954b6707bddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape (10000, 30)\n",
            "Y.shape (10000, 10)\n",
            "Xoh.shape (10000, 30, 37)\n",
            "Yoh.shape (10000, 10, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y[:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXTbeYi-463s",
        "outputId": "a0b3b54c-b171-4477-8ff1-8cc58aa8aaa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2, 10, 10,  9,  0,  1,  6,  0,  1, 10]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Yoh[:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrynWX4v4Dk-",
        "outputId": "d9a75dbf-b2c5-44ac-bc30-db825f93a399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0\n",
        "\n",
        "print(\"Source date:\",dataset[index][0])\n",
        "print(\"Target date:\",dataset[index][1])\n",
        "print()\n",
        "print(\"Source after preprocessing (indices):\", X[index])\n",
        "print(\"Target after preprocessing (indices):\", Y[index])\n",
        "print()\n",
        "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
        "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
      ],
      "metadata": {
        "id": "WOjC0KX14pat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "857eb2b5-2f86-4f70-d216-dfe6a897cc6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source date: 9 may 1998\n",
            "Target date: 1998-05-09\n",
            "\n",
            "Source after preprocessing (indices): [12  0 24 13 34  0  4 12 12 11 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n",
            " 36 36 36 36 36 36]\n",
            "Target after preprocessing (indices): [ 2 10 10  9  0  1  6  0  1 10]\n",
            "\n",
            "Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n",
            "Target after preprocessing (one-hot): [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repeator = RepeatVector(Tx)\n",
        "concatenator = Concatenate(axis = -1)\n",
        "\n",
        "densor1 = Dense(units = 10, activation = \"tanh\")\n",
        "densor2 = Dense(units = 1, activation = \"relu\")\n",
        "\n",
        "activator = Activation(softmax, name = \"attention_weights\")\n",
        "dotor = Dot(axes = 1)"
      ],
      "metadata": {
        "id": "uHhNwWRnMULP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_step_attention(a, s_prev):\n",
        "\n",
        "  s_prev = repeator(s_prev)\n",
        "\n",
        "  concat = concatenator([a, s_prev])\n",
        "\n",
        "  e = densor1(concat)\n",
        "\n",
        "  energies = densor2(e)\n",
        "\n",
        "  alphas = activator(energies)\n",
        "\n",
        "  context = dotor([alphas,a])\n",
        "\n",
        "  return context"
      ],
      "metadata": {
        "id": "a2qmGh5bOolo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNIT TEST\n",
        "def one_step_attention_test(target):\n",
        "\n",
        "    m = 10\n",
        "    Tx = 30\n",
        "    n_a = 32\n",
        "    n_s = 64\n",
        "    #np.random.seed(10)\n",
        "    a = np.random.uniform(1, 0, (m, Tx, 2 * n_a)).astype(np.float32)\n",
        "    s_prev =np.random.uniform(1, 0, (m, n_s)).astype(np.float32) * 1\n",
        "    context = target(a, s_prev)\n",
        "\n",
        "    print(\"\\033[92mAll tests passed!\")\n",
        "\n",
        "one_step_attention_test(one_step_attention)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUy7Vu92QMXD",
        "outputId": "bbf34a85-bf5d-4ad0-8023-ed994a6bbd1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92mAll tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_a = 32 #number of units for pre-attention, bidirectional LSTM's hidden state \"a\"\n",
        "n_s = 64 #number of units for post-attention LSTM's hidden state \"s\"\n",
        "\n",
        "post_activation_LSTM_cell = LSTM(n_s, return_state = True) # Please do not modify this global variable.\n",
        "output_layer = Dense(len(machine_vocab), activation=softmax)"
      ],
      "metadata": {
        "id": "zjc2Uc0cQP_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def modelf(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
        "\n",
        "  X = Input(shape = (Tx, human_vocab_size))\n",
        "  s0 = Input(shape = (n_s, ), name = \"s0\")\n",
        "  c0 = Input(shape = (n_s, ), name = \"c0\")\n",
        "\n",
        "  s = s0\n",
        "  c = c0\n",
        "\n",
        "  outputs = []\n",
        "\n",
        "  # Step 1: Define your pre-attention Bi-LSTM.\n",
        "  a = Bidirectional(LSTM(n_a, return_sequences = True))(X)\n",
        "\n",
        "  for t in range(Ty):\n",
        "\n",
        "    context = one_step_attention(a, s)\n",
        "\n",
        "    s, _, c = post_activation_LSTM_cell(context, initial_state = [s, c])\n",
        "\n",
        "    out = output_layer(s)\n",
        "\n",
        "    outputs.append(out)\n",
        "\n",
        "  model = Model(inputs = [X, s0, c0], outputs = outputs)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "c6vtNXzGSD43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def modelf_test(target):\n",
        "    m = 10\n",
        "    Tx = 30\n",
        "    n_a = 32\n",
        "    n_s = 64\n",
        "    len_human_vocab = 37\n",
        "    len_machine_vocab = 11\n",
        "\n",
        "\n",
        "    model = target(Tx, Ty, n_a, n_s, len_human_vocab, len_machine_vocab)\n",
        "\n",
        "    print(summary(model))\n",
        "\n",
        "\n",
        "    expected_summary = [['InputLayer', [(None, 30, 37)], 0],\n",
        "                         ['InputLayer', [(None, 64)], 0],\n",
        "                         ['Bidirectional', (None, 30, 64), 17920],\n",
        "                         ['RepeatVector', (None, 30, 64), 0, 30],\n",
        "                         ['Concatenate', (None, 30, 128), 0],\n",
        "                         ['Dense', (None, 30, 10), 1290, 'tanh'],\n",
        "                         ['Dense', (None, 30, 1), 11, 'relu'],\n",
        "                         ['Activation', (None, 30, 1), 0],\n",
        "                         ['Dot', (None, 1, 64), 0],\n",
        "                         ['InputLayer', [(None, 64)], 0],\n",
        "                         ['LSTM',[(None, 64), (None, 64), (None, 64)], 33024,[(None, 1, 64), (None, 64), (None, 64)],'tanh'],\n",
        "                         ['Dense', (None, 11), 715, 'softmax']]\n",
        "\n",
        "    comparator(summary(model), expected_summary)\n",
        "\n",
        "\n",
        "modelf_test(modelf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av3WGLJ2T7ro",
        "outputId": "7307b36c-ce95-493f-bd15-34c45b688bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['InputLayer', [(None, 30, 37)], 0], ['InputLayer', [(None, 64)], 0], ['Bidirectional', (None, 30, 64), 17920], ['RepeatVector', (None, 30, 64), 0, 30], ['Concatenate', (None, 30, 128), 0], ['Dense', (None, 30, 10), 1290, 'tanh'], ['Dense', (None, 30, 1), 11, 'relu'], ['Activation', (None, 30, 1), 0], ['Dot', (None, 1, 64), 0], ['InputLayer', [(None, 64)], 0], ['LSTM', [(None, 64), (None, 64), (None, 64)], 33024, [(None, 1, 64), (None, 64), (None, 64)], 'tanh'], ['Dense', (None, 11), 715, 'softmax']]\n",
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = modelf(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
      ],
      "metadata": {
        "id": "e67cNYqNUOuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8mKFZaWUtez",
        "outputId": "7436d2c1-557f-46c0-a8bf-5d10e46eaede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 30, 37)]     0           []                               \n",
            "                                                                                                  \n",
            " s0 (InputLayer)                [(None, 64)]         0           []                               \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirectional  (None, 30, 64)      17920       ['input_2[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " repeat_vector (RepeatVector)   (None, 30, 64)       0           ['s0[0][0]',                     \n",
            "                                                                  'lstm[10][0]',                  \n",
            "                                                                  'lstm[11][0]',                  \n",
            "                                                                  'lstm[12][0]',                  \n",
            "                                                                  'lstm[13][0]',                  \n",
            "                                                                  'lstm[14][0]',                  \n",
            "                                                                  'lstm[15][0]',                  \n",
            "                                                                  'lstm[16][0]',                  \n",
            "                                                                  'lstm[17][0]',                  \n",
            "                                                                  'lstm[18][0]']                  \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 30, 128)      0           ['bidirectional_1[0][0]',        \n",
            "                                                                  'repeat_vector[10][0]',         \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'repeat_vector[11][0]',         \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'repeat_vector[12][0]',         \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'repeat_vector[13][0]',         \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'repeat_vector[14][0]',         \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'repeat_vector[15][0]',         \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'repeat_vector[16][0]',         \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'repeat_vector[17][0]',         \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'repeat_vector[18][0]',         \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'repeat_vector[19][0]']         \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 30, 10)       1290        ['concatenate[10][0]',           \n",
            "                                                                  'concatenate[11][0]',           \n",
            "                                                                  'concatenate[12][0]',           \n",
            "                                                                  'concatenate[13][0]',           \n",
            "                                                                  'concatenate[14][0]',           \n",
            "                                                                  'concatenate[15][0]',           \n",
            "                                                                  'concatenate[16][0]',           \n",
            "                                                                  'concatenate[17][0]',           \n",
            "                                                                  'concatenate[18][0]',           \n",
            "                                                                  'concatenate[19][0]']           \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 30, 1)        11          ['dense[10][0]',                 \n",
            "                                                                  'dense[11][0]',                 \n",
            "                                                                  'dense[12][0]',                 \n",
            "                                                                  'dense[13][0]',                 \n",
            "                                                                  'dense[14][0]',                 \n",
            "                                                                  'dense[15][0]',                 \n",
            "                                                                  'dense[16][0]',                 \n",
            "                                                                  'dense[17][0]',                 \n",
            "                                                                  'dense[18][0]',                 \n",
            "                                                                  'dense[19][0]']                 \n",
            "                                                                                                  \n",
            " attention_weights (Activation)  (None, 30, 1)       0           ['dense_1[10][0]',               \n",
            "                                                                  'dense_1[11][0]',               \n",
            "                                                                  'dense_1[12][0]',               \n",
            "                                                                  'dense_1[13][0]',               \n",
            "                                                                  'dense_1[14][0]',               \n",
            "                                                                  'dense_1[15][0]',               \n",
            "                                                                  'dense_1[16][0]',               \n",
            "                                                                  'dense_1[17][0]',               \n",
            "                                                                  'dense_1[18][0]',               \n",
            "                                                                  'dense_1[19][0]']               \n",
            "                                                                                                  \n",
            " dot (Dot)                      (None, 1, 64)        0           ['attention_weights[10][0]',     \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'attention_weights[11][0]',     \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'attention_weights[12][0]',     \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'attention_weights[13][0]',     \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'attention_weights[14][0]',     \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'attention_weights[15][0]',     \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'attention_weights[16][0]',     \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'attention_weights[17][0]',     \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'attention_weights[18][0]',     \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'attention_weights[19][0]',     \n",
            "                                                                  'bidirectional_1[0][0]']        \n",
            "                                                                                                  \n",
            " c0 (InputLayer)                [(None, 64)]         0           []                               \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 64),         33024       ['dot[10][0]',                   \n",
            "                                 (None, 64),                      's0[0][0]',                     \n",
            "                                 (None, 64)]                      'c0[0][0]',                     \n",
            "                                                                  'dot[11][0]',                   \n",
            "                                                                  'lstm[10][0]',                  \n",
            "                                                                  'lstm[10][2]',                  \n",
            "                                                                  'dot[12][0]',                   \n",
            "                                                                  'lstm[11][0]',                  \n",
            "                                                                  'lstm[11][2]',                  \n",
            "                                                                  'dot[13][0]',                   \n",
            "                                                                  'lstm[12][0]',                  \n",
            "                                                                  'lstm[12][2]',                  \n",
            "                                                                  'dot[14][0]',                   \n",
            "                                                                  'lstm[13][0]',                  \n",
            "                                                                  'lstm[13][2]',                  \n",
            "                                                                  'dot[15][0]',                   \n",
            "                                                                  'lstm[14][0]',                  \n",
            "                                                                  'lstm[14][2]',                  \n",
            "                                                                  'dot[16][0]',                   \n",
            "                                                                  'lstm[15][0]',                  \n",
            "                                                                  'lstm[15][2]',                  \n",
            "                                                                  'dot[17][0]',                   \n",
            "                                                                  'lstm[16][0]',                  \n",
            "                                                                  'lstm[16][2]',                  \n",
            "                                                                  'dot[18][0]',                   \n",
            "                                                                  'lstm[17][0]',                  \n",
            "                                                                  'lstm[17][2]',                  \n",
            "                                                                  'dot[19][0]',                   \n",
            "                                                                  'lstm[18][0]',                  \n",
            "                                                                  'lstm[18][2]']                  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 11)           715         ['lstm[10][0]',                  \n",
            "                                                                  'lstm[11][0]',                  \n",
            "                                                                  'lstm[12][0]',                  \n",
            "                                                                  'lstm[13][0]',                  \n",
            "                                                                  'lstm[14][0]',                  \n",
            "                                                                  'lstm[15][0]',                  \n",
            "                                                                  'lstm[16][0]',                  \n",
            "                                                                  'lstm[17][0]',                  \n",
            "                                                                  'lstm[18][0]',                  \n",
            "                                                                  'lstm[19][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 52,960\n",
            "Trainable params: 52,960\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.005,\n",
        "    decay_steps=10000,  # Adjust the decay steps as needed\n",
        "    decay_rate=0.9\n",
        ")\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999)"
      ],
      "metadata": {
        "id": "QlXdDCGNVuDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = opt, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "7GtSOT9bVXn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s0 = np.zeros((m, n_s))\n",
        "c0 = np.zeros((m, n_s))\n",
        "outputs = list(Yoh.swapaxes(0,1))"
      ],
      "metadata": {
        "id": "5M-1-BWyV0uJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([Xoh, s0, c0], outputs, epochs=5, batch_size=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9eqYkGwWf7b",
        "outputId": "d21eea61-1aee-4a95-806d-6e7f806e0028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "100/100 [==============================] - 45s 100ms/step - loss: 15.9302 - dense_2_loss: 1.1241 - dense_2_1_loss: 0.9640 - dense_2_2_loss: 1.7587 - dense_2_3_loss: 2.6146 - dense_2_4_loss: 0.7180 - dense_2_5_loss: 1.1783 - dense_2_6_loss: 2.5673 - dense_2_7_loss: 0.8383 - dense_2_8_loss: 1.6209 - dense_2_9_loss: 2.5459 - dense_2_accuracy: 0.5315 - dense_2_1_accuracy: 0.7307 - dense_2_2_accuracy: 0.3033 - dense_2_3_accuracy: 0.0958 - dense_2_4_accuracy: 0.8803 - dense_2_5_accuracy: 0.3869 - dense_2_6_accuracy: 0.0935 - dense_2_7_accuracy: 0.8800 - dense_2_8_accuracy: 0.3050 - dense_2_9_accuracy: 0.1236\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 12s 115ms/step - loss: 8.3121 - dense_2_loss: 0.1266 - dense_2_1_loss: 0.1124 - dense_2_2_loss: 1.0436 - dense_2_3_loss: 2.0450 - dense_2_4_loss: 0.0139 - dense_2_5_loss: 0.1974 - dense_2_6_loss: 1.6266 - dense_2_7_loss: 0.0085 - dense_2_8_loss: 1.0109 - dense_2_9_loss: 2.1272 - dense_2_accuracy: 0.9615 - dense_2_1_accuracy: 0.9639 - dense_2_2_accuracy: 0.5236 - dense_2_3_accuracy: 0.2526 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9460 - dense_2_6_accuracy: 0.3973 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.5693 - dense_2_9_accuracy: 0.2113\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 12s 118ms/step - loss: 6.2112 - dense_2_loss: 0.0817 - dense_2_1_loss: 0.0713 - dense_2_2_loss: 0.7452 - dense_2_3_loss: 1.4899 - dense_2_4_loss: 0.0060 - dense_2_5_loss: 0.1097 - dense_2_6_loss: 1.0150 - dense_2_7_loss: 0.0079 - dense_2_8_loss: 0.8186 - dense_2_9_loss: 1.8661 - dense_2_accuracy: 0.9699 - dense_2_1_accuracy: 0.9731 - dense_2_2_accuracy: 0.7000 - dense_2_3_accuracy: 0.4539 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9706 - dense_2_6_accuracy: 0.6383 - dense_2_7_accuracy: 0.9999 - dense_2_8_accuracy: 0.6633 - dense_2_9_accuracy: 0.3024\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 12s 120ms/step - loss: 3.5019 - dense_2_loss: 0.0617 - dense_2_1_loss: 0.0599 - dense_2_2_loss: 0.4088 - dense_2_3_loss: 0.6397 - dense_2_4_loss: 0.0039 - dense_2_5_loss: 0.0883 - dense_2_6_loss: 0.6721 - dense_2_7_loss: 0.0069 - dense_2_8_loss: 0.5716 - dense_2_9_loss: 0.9890 - dense_2_accuracy: 0.9763 - dense_2_1_accuracy: 0.9775 - dense_2_2_accuracy: 0.8248 - dense_2_3_accuracy: 0.7936 - dense_2_4_accuracy: 0.9998 - dense_2_5_accuracy: 0.9716 - dense_2_6_accuracy: 0.7792 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.7753 - dense_2_9_accuracy: 0.6524\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 13s 134ms/step - loss: 1.8216 - dense_2_loss: 0.0448 - dense_2_1_loss: 0.0389 - dense_2_2_loss: 0.2730 - dense_2_3_loss: 0.2433 - dense_2_4_loss: 0.0018 - dense_2_5_loss: 0.0592 - dense_2_6_loss: 0.3985 - dense_2_7_loss: 0.0047 - dense_2_8_loss: 0.3418 - dense_2_9_loss: 0.4157 - dense_2_accuracy: 0.9811 - dense_2_1_accuracy: 0.9842 - dense_2_2_accuracy: 0.8537 - dense_2_3_accuracy: 0.9388 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9830 - dense_2_6_accuracy: 0.8773 - dense_2_7_accuracy: 0.9998 - dense_2_8_accuracy: 0.8734 - dense_2_9_accuracy: 0.8677\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7b614e5da380>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
        "s00 = np.zeros((1, n_s))\n",
        "c00 = np.zeros((1, n_s))\n",
        "for example in EXAMPLES:\n",
        "    source = string_to_int(example, Tx, human_vocab)\n",
        "    #print(source)\n",
        "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
        "    source = np.swapaxes(source, 0, 1)\n",
        "    source = np.expand_dims(source, axis=0)\n",
        "    prediction = model.predict([source, s00, c00])\n",
        "    prediction = np.argmax(prediction, axis = -1)\n",
        "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
        "    print(\"source:\", example)\n",
        "    print(\"output:\", ''.join(output),\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncfqxnFDWiC5",
        "outputId": "5eb13636-ef2c-40cd-9299-804b6d957d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 11s 11s/step\n",
            "source: 3 May 1979\n",
            "output: 1977-05-03 \n",
            "\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "source: 5 April 09\n",
            "output: 2009-04-04 \n",
            "\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "source: 21th of August 2016\n",
            "output: 2016-08-01 \n",
            "\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "source: Tue 10 Jul 2007\n",
            "output: 2007-07-00 \n",
            "\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "source: Saturday May 9 2018\n",
            "output: 2018-05-09 \n",
            "\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "source: March 3 2001\n",
            "output: 2001-03-03 \n",
            "\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "source: March 3rd 2001\n",
            "output: 2001-02-33 \n",
            "\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "source: 1 March 2001\n",
            "output: 2001-03-01 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7q9OC7pXYNcP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}